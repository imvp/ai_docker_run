services:
  app:
    build: ./app
    env_file: 'backend.env'
    ports:
      - "8501:8501"
    depends_on:
      - llm

  llm:
    image: ghcr.io/ggml-org/llama.cpp:server  # Using the official llama.cpp engine
    volumes:
      - ./models:/models
    ports:
      - "12434:8080"
    command: ["-m", "/models/Phi-4-mini-instruct-Q5_K_M.gguf", "--host", "0.0.0.0", "--port", "8080", "-c", "64000", "--log-verbosity", "0"]
